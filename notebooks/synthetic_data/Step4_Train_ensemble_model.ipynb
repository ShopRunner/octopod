{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Octopod Ensemble Model Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the fourth (and final) step of this tutorial, we will train an ensemble model using the image and text models we've already trained.\n",
    "\n",
    "This notebook was run on an AWS p3.2xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, BertTokenizer, get_cosine_schedule_with_warmup\n",
    "\n",
    "from octopod.learner import MultiTaskLearner, MultiInputMultiTaskLearner\n",
    "from octopod.dataloader import MultiDatasetLoader\n",
    "from octopod.ensemble import OctopodEnsembleDataset, BertResnetEnsembleForMultiTaskClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in train and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the csv's we created in Step 1.\n",
    "Remember to change the path if you stored your data somewhere other than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_COLOR_DF = pd.read_csv('data/color_swatches/color_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_COLOR_DF = pd.read_csv('data/color_swatches/color_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATTERN_DF = pd.read_csv('data/pattern_swatches/pattern_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_PATTERN_DF = pd.read_csv('data/pattern_swatches/pattern_valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will most likely have to alter this to however big your batches can be on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tok = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case=True\n",
    ")\n",
    "\n",
    "max_seq_length = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_train_dataset = OctopodEnsembleDataset(\n",
    "    text_inputs=TRAIN_COLOR_DF['complex_color'],\n",
    "    img_inputs=TRAIN_COLOR_DF['image_locs'],\n",
    "    y=TRAIN_COLOR_DF['simple_color_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length,\n",
    "    transform='train',\n",
    "    crop_transform='train'\n",
    "\n",
    ")\n",
    "color_valid_dataset = OctopodEnsembleDataset(\n",
    "    text_inputs=VALID_COLOR_DF['complex_color'],\n",
    "    img_inputs=VALID_COLOR_DF['image_locs'],\n",
    "    y=VALID_COLOR_DF['simple_color_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length,\n",
    "    transform='val',\n",
    "    crop_transform='val'\n",
    "\n",
    ")\n",
    "\n",
    "pattern_train_dataset = OctopodEnsembleDataset(\n",
    "    text_inputs=VALID_PATTERN_DF['fake_text'],\n",
    "    img_inputs=VALID_PATTERN_DF['image_locs'],\n",
    "    y=VALID_PATTERN_DF['pattern_type_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length,\n",
    "    transform='train',\n",
    "    crop_transform='train'\n",
    "\n",
    ")\n",
    "pattern_valid_dataset = OctopodEnsembleDataset(\n",
    "    text_inputs=VALID_PATTERN_DF['fake_text'],\n",
    "    img_inputs=VALID_PATTERN_DF['image_locs'],\n",
    "    y=VALID_PATTERN_DF['pattern_type_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length,\n",
    "    transform='val',\n",
    "    crop_transform='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then put the datasets into a dictionary of dataloaders.\n",
    "\n",
    "Each task is a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders_dict = {\n",
    "    'color': DataLoader(color_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "    'pattern': DataLoader(pattern_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "}\n",
    "valid_dataloaders_dict = {\n",
    "    'color': DataLoader(color_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "    'pattern': DataLoader(pattern_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLoader = MultiDatasetLoader(loader_dict=train_dataloaders_dict)\n",
    "len(TrainLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ValidLoader = MultiDatasetLoader(\n",
    "    loader_dict=valid_dataloaders_dict,\n",
    "    shuffle=False\n",
    ")\n",
    "len(ValidLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model and Learner\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the image model could potentially have multiple Resnets for different subsets of tasks, we need to create an `image_task_dict` that splits up the tasks grouped by the Resnet they use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_task_dict = {\n",
    "    'color_pattern': {\n",
    "        'color': TRAIN_COLOR_DF['simple_color_cat'].nunique(),\n",
    "        'pattern': TRAIN_PATTERN_DF['pattern_type_cat'].nunique()\n",
    "    }  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to create the `new_task_dict` for the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_task_dict = {\n",
    "    'color': TRAIN_COLOR_DF['simple_color_cat'].nunique(),\n",
    "    'pattern': TRAIN_PATTERN_DF['pattern_type_cat'].nunique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize the model by setting up the right shape with the image_task_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertResnetEnsembleForMultiTaskClassification(\n",
    "    image_task_dict=image_task_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load in the existing models by specifying the folder where the models live and their id's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model_id_dict = {\n",
    "    'color_pattern': 'IMAGE_MODEL1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_core_models(\n",
    "    folder='models/',\n",
    "    bert_model_id='TEXT_MODEL1',\n",
    "    resnet_model_id_dict=resnet_model_id_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've set some helper methods that will freeze the core bert and resnets for you if you only want to train the new layers. As with all other aspects of training, this is likely to require some experimentation to determine what works for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will likely need to explore different values in this section to find some that work\n",
    "for your particular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_bert()\n",
    "model.freeze_resnets()\n",
    "\n",
    "lr_last = 1e-3\n",
    "lr_main = 1e-5\n",
    "\n",
    "lr_list = [\n",
    "    {'params': model.bert.parameters(), 'lr': lr_main},\n",
    "    {'params': model.dropout.parameters(), 'lr': lr_main},   \n",
    "    {'params': model.image_resnets.parameters(), 'lr': lr_main},\n",
    "    {'params': model.image_dense_layers.parameters(), 'lr': lr_main},\n",
    "    {'params': model.ensemble_layers.parameters(), 'lr': lr_last},\n",
    "    {'params': model.classifiers.parameters(), 'lr': lr_last},\n",
    "]\n",
    "\n",
    "optimizer = optim.Adam(lr_list)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size= 4, gamma= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_dict = {'color': 'categorical_cross_entropy', 'pattern': 'categorical_cross_entropy'}\n",
    "metric_function_dict = {'color': 'multi_class_acc', 'pattern': 'multi_class_acc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = MultiInputMultiTaskLearner(model, TrainLoader, ValidLoader, new_task_dict, loss_function_dict, metric_function_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your model trains, you can see some output of how the model is performing overall and how it is doing on each individual task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>color_train_loss</th>\n",
       "      <th>color_val_loss</th>\n",
       "      <th>color_multi_class_accuracy</th>\n",
       "      <th>pattern_train_loss</th>\n",
       "      <th>pattern_val_loss</th>\n",
       "      <th>pattern_multi_class_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.255081</td>\n",
       "      <td>0.057677</td>\n",
       "      <td>0.216981</td>\n",
       "      <td>0.027293</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.747331</td>\n",
       "      <td>0.188935</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.181422</td>\n",
       "      <td>0.025467</td>\n",
       "      <td>0.159878</td>\n",
       "      <td>0.013501</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>0.459776</td>\n",
       "      <td>0.077156</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.216635</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>0.188308</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.582625</td>\n",
       "      <td>0.036265</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.164598</td>\n",
       "      <td>0.026894</td>\n",
       "      <td>0.154966</td>\n",
       "      <td>0.014742</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>0.289053</td>\n",
       "      <td>0.079393</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.167650</td>\n",
       "      <td>0.024614</td>\n",
       "      <td>0.163939</td>\n",
       "      <td>0.012435</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.215590</td>\n",
       "      <td>0.077229</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.159685</td>\n",
       "      <td>0.022670</td>\n",
       "      <td>0.153458</td>\n",
       "      <td>0.011310</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.240137</td>\n",
       "      <td>0.071746</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.117357</td>\n",
       "      <td>0.022851</td>\n",
       "      <td>0.111142</td>\n",
       "      <td>0.009109</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.197653</td>\n",
       "      <td>0.082217</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.118633</td>\n",
       "      <td>0.023313</td>\n",
       "      <td>0.112404</td>\n",
       "      <td>0.011147</td>\n",
       "      <td>0.898148</td>\n",
       "      <td>0.199106</td>\n",
       "      <td>0.075873</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.135601</td>\n",
       "      <td>0.021864</td>\n",
       "      <td>0.122261</td>\n",
       "      <td>0.010357</td>\n",
       "      <td>0.935185</td>\n",
       "      <td>0.307941</td>\n",
       "      <td>0.071577</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.164267</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.159604</td>\n",
       "      <td>0.011967</td>\n",
       "      <td>0.935185</td>\n",
       "      <td>0.224515</td>\n",
       "      <td>0.074537</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 best model saved with loss of 0.018583763390779495\n"
     ]
    }
   ],
   "source": [
    "learn.fit(\n",
    "    num_epochs=10,\n",
    "    scheduler=exp_lr_scheduler,\n",
    "    step_scheduler_on_batch=False,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    best_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally the ensemble would perform better than either the image or text model alone, but our performance is probably suffering due to this being synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking validation data\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a method on the learner called `get_val_preds`, which makes predictions on the validation data. You can then use this to analyze your model's performance in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_dict = learn.get_val_preds(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'color': {'y_true': array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "         0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1]),\n",
       "  'y_pred': array([[0.01048604, 0.989514  ],\n",
       "         [0.00740035, 0.9925997 ],\n",
       "         [0.04395323, 0.9560468 ],\n",
       "         [0.00924578, 0.99075425],\n",
       "         [0.01007592, 0.98992413],\n",
       "         [0.28866163, 0.7113384 ],\n",
       "         [0.00713036, 0.9928697 ],\n",
       "         [0.00621065, 0.9937894 ],\n",
       "         [0.9387955 , 0.06120452],\n",
       "         [0.03254075, 0.96745926],\n",
       "         [0.78577715, 0.21422282],\n",
       "         [0.6562214 , 0.34377858],\n",
       "         [0.00641926, 0.9935807 ],\n",
       "         [0.00988812, 0.9901119 ],\n",
       "         [0.814158  , 0.18584193],\n",
       "         [0.02132111, 0.9786789 ],\n",
       "         [0.00856477, 0.9914352 ],\n",
       "         [0.00637868, 0.9936213 ],\n",
       "         [0.01042844, 0.9895716 ],\n",
       "         [0.03716977, 0.9628302 ],\n",
       "         [0.00939967, 0.99060035],\n",
       "         [0.84245634, 0.15754372],\n",
       "         [0.9054054 , 0.09459465],\n",
       "         [0.7338673 , 0.2661327 ],\n",
       "         [0.00817555, 0.99182445],\n",
       "         [0.00923563, 0.9907644 ],\n",
       "         [0.02423073, 0.9757692 ],\n",
       "         [0.6272438 , 0.37275615],\n",
       "         [0.2513795 , 0.7486205 ],\n",
       "         [0.80155087, 0.19844918],\n",
       "         [0.00568755, 0.9943124 ],\n",
       "         [0.97824436, 0.02175561],\n",
       "         [0.00595862, 0.9940414 ],\n",
       "         [0.78092563, 0.2190744 ],\n",
       "         [0.82504463, 0.17495546],\n",
       "         [0.00934243, 0.99065757],\n",
       "         [0.00636282, 0.9936372 ],\n",
       "         [0.21090114, 0.78909886],\n",
       "         [0.11628608, 0.8837139 ],\n",
       "         [0.00555313, 0.9944469 ],\n",
       "         [0.00697055, 0.9930294 ],\n",
       "         [0.05133883, 0.9486611 ],\n",
       "         [0.95697474, 0.04302521],\n",
       "         [0.00848031, 0.99151975],\n",
       "         [0.8279785 , 0.1720215 ],\n",
       "         [0.81128484, 0.18871516],\n",
       "         [0.89364135, 0.1063586 ],\n",
       "         [0.01118524, 0.9888148 ],\n",
       "         [0.01050298, 0.98949695],\n",
       "         [0.0058312 , 0.99416876],\n",
       "         [0.00809296, 0.991907  ],\n",
       "         [0.0096035 , 0.9903965 ],\n",
       "         [0.7888523 , 0.21114774],\n",
       "         [0.00678096, 0.9932191 ],\n",
       "         [0.00655116, 0.9934489 ],\n",
       "         [0.00639613, 0.9936039 ],\n",
       "         [0.01056043, 0.98943955],\n",
       "         [0.01112582, 0.9888742 ],\n",
       "         [0.00562764, 0.99437237],\n",
       "         [0.7303985 , 0.26960155],\n",
       "         [0.94157356, 0.05842642],\n",
       "         [0.00759348, 0.99240655],\n",
       "         [0.00751627, 0.9924837 ],\n",
       "         [0.6437531 , 0.35624695],\n",
       "         [0.86407465, 0.1359253 ],\n",
       "         [0.88013786, 0.11986218],\n",
       "         [0.00687421, 0.9931258 ],\n",
       "         [0.0068686 , 0.99313134],\n",
       "         [0.00546299, 0.99453694],\n",
       "         [0.0121013 , 0.98789865],\n",
       "         [0.9513504 , 0.04864962],\n",
       "         [0.8334349 , 0.16656515],\n",
       "         [0.00538547, 0.99461454],\n",
       "         [0.91318274, 0.0868173 ],\n",
       "         [0.7527642 , 0.2472358 ],\n",
       "         [0.01908102, 0.98091894],\n",
       "         [0.48082945, 0.5191706 ],\n",
       "         [0.01786678, 0.9821332 ],\n",
       "         [0.92456615, 0.07543386],\n",
       "         [0.37496567, 0.62503433],\n",
       "         [0.54210526, 0.45789474],\n",
       "         [0.02675085, 0.9732492 ],\n",
       "         [0.8484366 , 0.1515634 ],\n",
       "         [0.00676199, 0.993238  ],\n",
       "         [0.01073523, 0.9892647 ],\n",
       "         [0.009264  , 0.99073595],\n",
       "         [0.00514109, 0.994859  ],\n",
       "         [0.00654112, 0.9934589 ],\n",
       "         [0.7991223 , 0.20087771],\n",
       "         [0.00503788, 0.9949621 ],\n",
       "         [0.9446887 , 0.05531133],\n",
       "         [0.93437517, 0.0656249 ],\n",
       "         [0.8716069 , 0.12839316],\n",
       "         [0.60406786, 0.3959322 ],\n",
       "         [0.00556932, 0.9944307 ],\n",
       "         [0.03105773, 0.9689423 ],\n",
       "         [0.01842988, 0.9815701 ],\n",
       "         [0.29955468, 0.7004453 ],\n",
       "         [0.17241745, 0.8275826 ],\n",
       "         [0.8168618 , 0.18313822],\n",
       "         [0.00589457, 0.99410546],\n",
       "         [0.01566603, 0.98433393],\n",
       "         [0.01032303, 0.989677  ],\n",
       "         [0.01215988, 0.9878401 ],\n",
       "         [0.01786776, 0.9821323 ],\n",
       "         [0.02335131, 0.97664875],\n",
       "         [0.82916504, 0.170835  ],\n",
       "         [0.00589154, 0.99410844]], dtype=float32)},\n",
       " 'pattern': {'y_true': array([0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "         1, 0, 0]),\n",
       "  'y_pred': array([[0.5551021 , 0.44489792],\n",
       "         [0.2503276 , 0.7496724 ],\n",
       "         [0.21374336, 0.7862566 ],\n",
       "         [0.11249155, 0.8875084 ],\n",
       "         [0.6692096 , 0.3307904 ],\n",
       "         [0.1317903 , 0.8682098 ],\n",
       "         [0.69712   , 0.30288005],\n",
       "         [0.30971047, 0.6902895 ],\n",
       "         [0.1056269 , 0.8943731 ],\n",
       "         [0.12224168, 0.8777583 ],\n",
       "         [0.44325483, 0.5567451 ],\n",
       "         [0.65924937, 0.34075063],\n",
       "         [0.25930333, 0.74069667],\n",
       "         [0.5409025 , 0.45909753],\n",
       "         [0.16331965, 0.83668035],\n",
       "         [0.21877758, 0.78122234],\n",
       "         [0.12910292, 0.8708971 ],\n",
       "         [0.4291098 , 0.5708902 ],\n",
       "         [0.26961187, 0.7303881 ],\n",
       "         [0.3878999 , 0.6121001 ],\n",
       "         [0.6057971 , 0.39420295],\n",
       "         [0.49406394, 0.50593597],\n",
       "         [0.11352569, 0.8864743 ],\n",
       "         [0.6042826 , 0.39571732],\n",
       "         [0.5280275 , 0.47197255]], dtype=float32)}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Export Model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble model can also be saved or exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(folder='models/', model_id='ENSEMBLE_MODEL1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(folder='models/', model_id='ENSEMBLE_MODEL1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
