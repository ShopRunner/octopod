{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tonks Ensemble Model Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the fourth (and final) step of this tutorial, we will train an ensemble model using the image and text models we've already trained.\n",
    "\n",
    "This notebook was run on an AWS p3.2xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, BertTokenizer, get_cosine_schedule_with_warmup\n",
    "\n",
    "from tonks.learner import MultiTaskLearner, MultiInputMultiTaskLearner\n",
    "from tonks.dataloader import MultiDatasetLoader\n",
    "from tonks.ensemble import TonksEnsembleDataset, BertResnetEnsembleForMultiTaskClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in train and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the csv's we created in Step 1.\n",
    "Remember to change the path if you stored your data somewhere other than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_COLOR_DF = pd.read_csv('data/color_swatches/color_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_COLOR_DF = pd.read_csv('data/color_swatches/color_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATTERN_DF = pd.read_csv('data/pattern_swatches/pattern_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_PATTERN_DF = pd.read_csv('data/pattern_swatches/pattern_valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will most likely have to alter this to however big your batches can be on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tok = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case=True\n",
    ")\n",
    "\n",
    "max_seq_length = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_train_dataset = TonksEnsembleDataset(\n",
    "    text_inputs=TRAIN_COLOR_DF['complex_color'],\n",
    "    img_inputs=TRAIN_COLOR_DF['image_locs'],\n",
    "    y=TRAIN_COLOR_DF['simple_color_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length,\n",
    "    transform='train',\n",
    "    crop_transform='train'\n",
    "\n",
    ")\n",
    "color_valid_dataset = TonksEnsembleDataset(\n",
    "    text_inputs=VALID_COLOR_DF['complex_color'],\n",
    "    img_inputs=VALID_COLOR_DF['image_locs'],\n",
    "    y=VALID_COLOR_DF['simple_color_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length,\n",
    "    transform='val',\n",
    "    crop_transform='val'\n",
    "\n",
    ")\n",
    "\n",
    "pattern_train_dataset = TonksEnsembleDataset(\n",
    "    text_inputs=VALID_PATTERN_DF['fake_text'],\n",
    "    img_inputs=VALID_PATTERN_DF['image_locs'],\n",
    "    y=VALID_PATTERN_DF['pattern_type_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length,\n",
    "    transform='train',\n",
    "    crop_transform='train'\n",
    "\n",
    ")\n",
    "pattern_valid_dataset = TonksEnsembleDataset(\n",
    "    text_inputs=VALID_PATTERN_DF['fake_text'],\n",
    "    img_inputs=VALID_PATTERN_DF['image_locs'],\n",
    "    y=VALID_PATTERN_DF['pattern_type_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length,\n",
    "    transform='val',\n",
    "    crop_transform='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then put the datasets into a dictionary of dataloaders.\n",
    "\n",
    "Each task is a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders_dict = {\n",
    "    'color': DataLoader(color_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "    'pattern': DataLoader(pattern_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "}\n",
    "valid_dataloaders_dict = {\n",
    "    'color': DataLoader(color_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "    'pattern': DataLoader(pattern_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLoader = MultiDatasetLoader(loader_dict=train_dataloaders_dict)\n",
    "len(TrainLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ValidLoader = MultiDatasetLoader(\n",
    "    loader_dict=valid_dataloaders_dict,\n",
    "    shuffle=False\n",
    ")\n",
    "len(ValidLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model and Learner\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the image model could potentially have multiple Resnets for different subsets of tasks, we need to create an `image_task_dict` that splits up the tasks grouped by the Resnet they use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_task_dict = {\n",
    "    'color_pattern': {\n",
    "        'color': TRAIN_COLOR_DF['simple_color_cat'].nunique(),\n",
    "        'pattern': TRAIN_PATTERN_DF['pattern_type_cat'].nunique()\n",
    "    }  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to create the `new_task_dict` for the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_task_dict = {\n",
    "    'color': TRAIN_COLOR_DF['simple_color_cat'].nunique(),\n",
    "    'pattern': TRAIN_PATTERN_DF['pattern_type_cat'].nunique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize the model by setting up the right shape with the image_task_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertResnetEnsembleForMultiTaskClassification(\n",
    "    image_task_dict=image_task_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load in the existing models by specifying the folder where the models live and their id's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model_id_dict = {\n",
    "    'color_pattern': 'IMAGE_MODEL1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_core_models(\n",
    "    folder='models/',\n",
    "    bert_model_id='TEXT_MODEL1',\n",
    "    resnet_model_id_dict=resnet_model_id_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've set some helper methods that will freeze the core bert and resnets for you if you only want to train the new layers. As with all other aspects of training, this is likely to require some experimentation to determine what works for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will likely need to explore different values in this section to find some that work\n",
    "for your particular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_bert()\n",
    "model.freeze_resnets()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_last = 1e-3\n",
    "lr_main = 1e-5\n",
    "\n",
    "lr_list = [\n",
    "    {'params': model.bert.parameters(), 'lr': lr_main},\n",
    "    {'params': model.dropout.parameters(), 'lr': lr_main},   \n",
    "    {'params': model.image_resnets.parameters(), 'lr': lr_main},\n",
    "    {'params': model.image_dense_layers.parameters(), 'lr': lr_main},\n",
    "    {'params': model.ensemble_layers.parameters(), 'lr': lr_last},\n",
    "    {'params': model.classifiers.parameters(), 'lr': lr_last},\n",
    "]\n",
    "\n",
    "optimizer = optim.Adam(lr_list)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size= 4, gamma= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = MultiInputMultiTaskLearner(model, TrainLoader, ValidLoader, new_task_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your model trains, you can see some output of how the model is performing overall and how it is doing on each individual task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>color_train_loss</th>\n",
       "      <th>color_val_loss</th>\n",
       "      <th>color_acc</th>\n",
       "      <th>pattern_train_loss</th>\n",
       "      <th>pattern_val_loss</th>\n",
       "      <th>pattern_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.269334</td>\n",
       "      <td>0.372842</td>\n",
       "      <td>0.227687</td>\n",
       "      <td>0.264316</td>\n",
       "      <td>0.889908</td>\n",
       "      <td>0.809089</td>\n",
       "      <td>0.846015</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.223326</td>\n",
       "      <td>0.397131</td>\n",
       "      <td>0.184951</td>\n",
       "      <td>0.286414</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.720665</td>\n",
       "      <td>0.879854</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.202404</td>\n",
       "      <td>0.357665</td>\n",
       "      <td>0.172665</td>\n",
       "      <td>0.270390</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.587830</td>\n",
       "      <td>0.738185</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.232210</td>\n",
       "      <td>0.367504</td>\n",
       "      <td>0.207638</td>\n",
       "      <td>0.275364</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.550674</td>\n",
       "      <td>0.769239</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.133768</td>\n",
       "      <td>0.376696</td>\n",
       "      <td>0.097655</td>\n",
       "      <td>0.275917</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>0.601798</td>\n",
       "      <td>0.816090</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.147174</td>\n",
       "      <td>0.388062</td>\n",
       "      <td>0.121395</td>\n",
       "      <td>0.296142</td>\n",
       "      <td>0.862385</td>\n",
       "      <td>0.481277</td>\n",
       "      <td>0.788833</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.171947</td>\n",
       "      <td>0.370653</td>\n",
       "      <td>0.141226</td>\n",
       "      <td>0.272476</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.570095</td>\n",
       "      <td>0.798706</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.123373</td>\n",
       "      <td>0.375156</td>\n",
       "      <td>0.108670</td>\n",
       "      <td>0.278227</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.313924</td>\n",
       "      <td>0.797770</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.117283</td>\n",
       "      <td>0.361361</td>\n",
       "      <td>0.098623</td>\n",
       "      <td>0.260629</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.359123</td>\n",
       "      <td>0.800550</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.144764</td>\n",
       "      <td>0.368404</td>\n",
       "      <td>0.123345</td>\n",
       "      <td>0.265812</td>\n",
       "      <td>0.880734</td>\n",
       "      <td>0.422360</td>\n",
       "      <td>0.815706</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 best model saved with loss of 0.3576649023748156\n"
     ]
    }
   ],
   "source": [
    "learn.fit(\n",
    "    num_epochs=10,\n",
    "    loss_function=loss_function,\n",
    "    scheduler=exp_lr_scheduler,\n",
    "    step_scheduler_on_batch=False,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    best_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally the ensemble would perform better than either the image or text model alone, but our performance is probably suffering due to this being synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking validation data\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a method on the learner called `get_val_preds`, which makes predictions on the validation data. You can then use this to analyze your model's performance in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = learn.get_val_preds(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'color': {'y_true': array([1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1.,\n",
       "         0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "         1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "         1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "         1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "         0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "         1., 1., 0., 1., 1., 1., 1.]),\n",
       "  'y_pred': array([[2.41741794e-03, 9.97582555e-01],\n",
       "         [2.05460470e-03, 9.97945368e-01],\n",
       "         [2.27170484e-03, 9.97728288e-01],\n",
       "         [9.93301749e-01, 6.69822237e-03],\n",
       "         [1.75855076e-03, 9.98241425e-01],\n",
       "         [7.11645419e-03, 9.92883563e-01],\n",
       "         [8.93120281e-03, 9.91068721e-01],\n",
       "         [1.27694395e-03, 9.98723090e-01],\n",
       "         [1.07391253e-01, 8.92608702e-01],\n",
       "         [8.78677983e-03, 9.91213262e-01],\n",
       "         [9.64594662e-01, 3.54053192e-02],\n",
       "         [9.93765354e-01, 6.23465003e-03],\n",
       "         [6.99074648e-04, 9.99300957e-01],\n",
       "         [1.87253777e-03, 9.98127520e-01],\n",
       "         [9.95451748e-01, 4.54826141e-03],\n",
       "         [9.50329304e-01, 4.96707596e-02],\n",
       "         [1.50925887e-03, 9.98490691e-01],\n",
       "         [9.82128620e-01, 1.78713910e-02],\n",
       "         [1.59219712e-01, 8.40780318e-01],\n",
       "         [2.00875755e-03, 9.97991204e-01],\n",
       "         [1.78072776e-03, 9.98219311e-01],\n",
       "         [1.10144161e-01, 8.89855862e-01],\n",
       "         [6.13381271e-04, 9.99386549e-01],\n",
       "         [1.90491939e-03, 9.98095095e-01],\n",
       "         [1.51545717e-03, 9.98484552e-01],\n",
       "         [9.97648299e-01, 2.35166145e-03],\n",
       "         [9.27397609e-01, 7.26024285e-02],\n",
       "         [8.82880628e-01, 1.17119394e-01],\n",
       "         [6.71363592e-01, 3.28636408e-01],\n",
       "         [2.71909326e-01, 7.28090703e-01],\n",
       "         [9.40750778e-01, 5.92491738e-02],\n",
       "         [7.87024915e-01, 2.12975100e-01],\n",
       "         [5.56005053e-02, 9.44399536e-01],\n",
       "         [9.85661149e-01, 1.43389106e-02],\n",
       "         [1.82488654e-03, 9.98175144e-01],\n",
       "         [1.95730967e-03, 9.98042703e-01],\n",
       "         [9.93940830e-01, 6.05920749e-03],\n",
       "         [1.45670429e-01, 8.54329526e-01],\n",
       "         [8.02118448e-04, 9.99197900e-01],\n",
       "         [7.65263975e-01, 2.34736055e-01],\n",
       "         [8.25755969e-02, 9.17424440e-01],\n",
       "         [9.27427351e-01, 7.25726858e-02],\n",
       "         [1.75063161e-03, 9.98249412e-01],\n",
       "         [1.55131659e-03, 9.98448610e-01],\n",
       "         [9.53792095e-01, 4.62079011e-02],\n",
       "         [9.82012153e-01, 1.79878902e-02],\n",
       "         [6.47946307e-03, 9.93520558e-01],\n",
       "         [1.78638671e-03, 9.98213649e-01],\n",
       "         [2.30308413e-03, 9.97696936e-01],\n",
       "         [9.62998450e-01, 3.70015278e-02],\n",
       "         [1.93592068e-03, 9.98064101e-01],\n",
       "         [7.39467505e-04, 9.99260485e-01],\n",
       "         [1.92937419e-01, 8.07062507e-01],\n",
       "         [1.63861818e-03, 9.98361409e-01],\n",
       "         [1.94106833e-03, 9.98058856e-01],\n",
       "         [2.24120193e-03, 9.97758746e-01],\n",
       "         [4.79038281e-04, 9.99521017e-01],\n",
       "         [6.12622360e-04, 9.99387383e-01],\n",
       "         [2.07710965e-03, 9.97922957e-01],\n",
       "         [9.89335954e-01, 1.06640747e-02],\n",
       "         [3.83328437e-03, 9.96166646e-01],\n",
       "         [9.86704111e-01, 1.32958647e-02],\n",
       "         [7.05097988e-02, 9.29490209e-01],\n",
       "         [9.95729148e-01, 4.27085767e-03],\n",
       "         [5.90364099e-01, 4.09635901e-01],\n",
       "         [9.93599534e-01, 6.40043896e-03],\n",
       "         [1.94581505e-03, 9.98054147e-01],\n",
       "         [1.65073760e-03, 9.98349309e-01],\n",
       "         [4.31246735e-04, 9.99568760e-01],\n",
       "         [9.78967011e-01, 2.10329760e-02],\n",
       "         [4.92988504e-04, 9.99506950e-01],\n",
       "         [9.30711687e-01, 6.92882538e-02],\n",
       "         [8.30880599e-04, 9.99169111e-01],\n",
       "         [1.02601103e-01, 8.97398949e-01],\n",
       "         [1.05592750e-01, 8.94407272e-01],\n",
       "         [1.99776003e-03, 9.98002231e-01],\n",
       "         [9.77344096e-01, 2.26558652e-02],\n",
       "         [6.69424713e-01, 3.30575228e-01],\n",
       "         [9.76794660e-01, 2.32053418e-02],\n",
       "         [8.67409170e-01, 1.32590801e-01],\n",
       "         [1.06324628e-03, 9.98936713e-01],\n",
       "         [1.13894872e-01, 8.86105061e-01],\n",
       "         [1.33430748e-03, 9.98665690e-01],\n",
       "         [9.34614897e-01, 6.53850436e-02],\n",
       "         [9.93742168e-01, 6.25784323e-03],\n",
       "         [9.94218469e-01, 5.78153227e-03],\n",
       "         [8.41053188e-01, 1.58946842e-01],\n",
       "         [1.70885921e-02, 9.82911348e-01],\n",
       "         [9.86679375e-01, 1.33205587e-02],\n",
       "         [1.64853851e-03, 9.98351455e-01],\n",
       "         [1.37284177e-03, 9.98627186e-01],\n",
       "         [9.91637170e-01, 8.36275890e-03],\n",
       "         [9.78686273e-01, 2.13136971e-02],\n",
       "         [6.21646345e-01, 3.78353596e-01],\n",
       "         [1.73844385e-03, 9.98261511e-01],\n",
       "         [9.96021450e-01, 3.97861982e-03],\n",
       "         [3.45310988e-03, 9.96546924e-01],\n",
       "         [4.46669408e-04, 9.99553263e-01],\n",
       "         [3.03014484e-03, 9.96969879e-01],\n",
       "         [1.97252724e-03, 9.98027503e-01],\n",
       "         [1.75246911e-03, 9.98247504e-01],\n",
       "         [9.15755033e-01, 8.42449740e-02],\n",
       "         [1.95058819e-03, 9.98049378e-01],\n",
       "         [1.91700074e-03, 9.98082995e-01],\n",
       "         [1.19259223e-01, 8.80740702e-01],\n",
       "         [1.97791518e-03, 9.98022079e-01],\n",
       "         [1.09842017e-01, 8.90157938e-01],\n",
       "         [1.88646722e-03, 9.98113513e-01],\n",
       "         [1.53108395e-03, 9.98468935e-01]])},\n",
       " 'pattern': {'y_true': array([0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
       "         0., 1., 0., 0., 0., 1., 0., 0.]),\n",
       "  'y_pred': array([[0.75561315, 0.24438682],\n",
       "         [0.70619476, 0.29380527],\n",
       "         [0.69901198, 0.30098802],\n",
       "         [0.7813049 , 0.21869516],\n",
       "         [0.74126023, 0.25873971],\n",
       "         [0.65098959, 0.34901035],\n",
       "         [0.86610186, 0.13389812],\n",
       "         [0.79217464, 0.20782542],\n",
       "         [0.7472676 , 0.25273234],\n",
       "         [0.75586748, 0.24413258],\n",
       "         [0.83653098, 0.163469  ],\n",
       "         [0.80041653, 0.19958341],\n",
       "         [0.72078335, 0.27921662],\n",
       "         [0.85437703, 0.14562291],\n",
       "         [0.74800241, 0.25199753],\n",
       "         [0.69596457, 0.30403543],\n",
       "         [0.7508232 , 0.24917684],\n",
       "         [0.81232208, 0.1876779 ],\n",
       "         [0.73762167, 0.26237836],\n",
       "         [0.88842779, 0.1115722 ],\n",
       "         [0.71423358, 0.28576639],\n",
       "         [0.84315717, 0.15684286],\n",
       "         [0.7296707 , 0.27032936],\n",
       "         [0.80239516, 0.19760484],\n",
       "         [0.8007496 , 0.19925037]])}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Export Model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble model can also be saved or exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(folder='models/', model_id='ENSEMBLE_MODEL1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(folder='models/', model_id='ENSEMBLE_MODEL1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
