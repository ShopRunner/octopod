{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the third step of this tutorial, we will train an text model. This notebook can be run in parallel with Step 2 (training the image model). A lof of the cells in this notebook are similar to the previous one.\n",
    "\n",
    "This notebook was run on an AWS p3.2xlarge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tonks Text Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, BertTokenizer, get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for text, we use the MultiTaskLearner since we will only have one input, the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonks import MultiTaskLearner, MultiDatasetLoader\n",
    "from tonks.text.dataset import TonksTextDataset\n",
    "from tonks.text.models.multi_task_bert import BertForMultiTaskClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Bert model, we need a tokenizer. We'll use the one from huggingface's `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tok = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in train and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the csv's we created in Step 1.\n",
    "Remember to change the path if you stored your data somewhere other than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_GENDER_DF = pd.read_csv('/home/ec2-user/fashion_dataset/gender_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_GENDER_DF = pd.read_csv('/home/ec2-user/fashion_dataset/gender_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SEASON_DF = pd.read_csv('/home/ec2-user/fashion_dataset/season_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SEASON_DF = pd.read_csv('/home/ec2-user/fashion_dataset/season_valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will most likely have to alter this to however big your batches can be on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `TonksTextDataSet` class to create train and valid datasets for each task.\n",
    "\n",
    "Check out the documentation for infomation about the `tokenizer` and `max_seq_length` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_train_dataset = TonksTextDataset(\n",
    "    x=TRAIN_GENDER_DF['productDisplayName'],\n",
    "    y=TRAIN_GENDER_DF['gender_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length\n",
    ")\n",
    "gender_valid_dataset = TonksTextDataset(\n",
    "    x=VALID_GENDER_DF['productDisplayName'],\n",
    "    y=VALID_GENDER_DF['gender_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length\n",
    ")\n",
    "\n",
    "season_train_dataset = TonksTextDataset(\n",
    "    x=TRAIN_SEASON_DF['productDisplayName'],\n",
    "    y=TRAIN_SEASON_DF['season_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length\n",
    ")\n",
    "season_valid_dataset = TonksTextDataset(\n",
    "    x=VALID_SEASON_DF['productDisplayName'],\n",
    "    y=VALID_SEASON_DF['season_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then put the datasets into a dictionary of dataloaders.\n",
    "\n",
    "Each task is a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders_dict = {\n",
    "    'gender': DataLoader(gender_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    'season': DataLoader(season_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "}\n",
    "valid_dataloaders_dict = {\n",
    "    'gender': DataLoader(gender_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4),\n",
    "    'season': DataLoader(season_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary of dataloaders is then put into an instance of the Tonks `MultiDatasetLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLoader = MultiDatasetLoader(loader_dict=train_dataloaders_dict)\n",
    "len(TrainLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ValidLoader = MultiDatasetLoader(loader_dict=valid_dataloaders_dict, shuffle=False)\n",
    "len(ValidLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a dictionary of the tasks and the number of unique values so that we can create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_task_dict = {\n",
    "    'gender': TRAIN_GENDER_DF['gender_cat'].nunique(),\n",
    "    'season': TRAIN_SEASON_DF['season_cat'].nunique(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': 5, 'season': 4}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_task_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model and Learner\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are completely new tasks so we use `new_task_dict`. If we had already trained a model on some tasks, we would use `pretrained_task_dict`.\n",
    "\n",
    "We are using the trained bert weights from the `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMultiTaskClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    new_task_dict=new_task_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will likely need to explore different values in this section to find some that work\n",
    "for your particular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "num_total_steps = len(TrainLoader)\n",
    "num_warmup_steps = int(len(TrainLoader) * 0.1)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=True)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_dict = {'gender': 'categorical_cross_entropy', 'season': 'categorical_cross_entropy'}\n",
    "metric_function_dict = {'gender': 'multi_class_acc', 'season': 'multi_class_acc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = MultiTaskLearner(model, TrainLoader, ValidLoader, new_task_dict, loss_function_dict, metric_function_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your model trains, you can see some output of how the model is performing overall and how it is doing on each individual task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>gender_train_loss</th>\n",
       "      <th>gender_val_loss</th>\n",
       "      <th>gender_multi_class_accuracy</th>\n",
       "      <th>season_train_loss</th>\n",
       "      <th>season_val_loss</th>\n",
       "      <th>season_multi_class_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1.587308</td>\n",
       "      <td>0.024508</td>\n",
       "      <td>1.616462</td>\n",
       "      <td>0.024402</td>\n",
       "      <td>0.411798</td>\n",
       "      <td>1.548419</td>\n",
       "      <td>0.024651</td>\n",
       "      <td>0.068028</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.293475</td>\n",
       "      <td>0.018337</td>\n",
       "      <td>1.256920</td>\n",
       "      <td>0.017395</td>\n",
       "      <td>0.524372</td>\n",
       "      <td>1.342238</td>\n",
       "      <td>0.019594</td>\n",
       "      <td>0.481153</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1.069644</td>\n",
       "      <td>0.014979</td>\n",
       "      <td>0.940256</td>\n",
       "      <td>0.011591</td>\n",
       "      <td>0.848475</td>\n",
       "      <td>1.242240</td>\n",
       "      <td>0.019499</td>\n",
       "      <td>0.481153</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.811460</td>\n",
       "      <td>0.011308</td>\n",
       "      <td>0.514079</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>0.909377</td>\n",
       "      <td>1.208151</td>\n",
       "      <td>0.018770</td>\n",
       "      <td>0.481153</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.684814</td>\n",
       "      <td>0.010080</td>\n",
       "      <td>0.315183</td>\n",
       "      <td>0.003763</td>\n",
       "      <td>0.923337</td>\n",
       "      <td>1.177884</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>0.481153</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.622825</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>0.218015</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.963188</td>\n",
       "      <td>1.162821</td>\n",
       "      <td>0.018277</td>\n",
       "      <td>0.483706</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.581187</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.155033</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.971406</td>\n",
       "      <td>1.149656</td>\n",
       "      <td>0.018124</td>\n",
       "      <td>0.487310</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.553077</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.117599</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.985478</td>\n",
       "      <td>1.133983</td>\n",
       "      <td>0.017772</td>\n",
       "      <td>0.497222</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.518735</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.093014</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.986829</td>\n",
       "      <td>1.086625</td>\n",
       "      <td>0.017490</td>\n",
       "      <td>0.477249</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.470713</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>0.077829</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.989193</td>\n",
       "      <td>0.994801</td>\n",
       "      <td>0.017399</td>\n",
       "      <td>0.499324</td>\n",
       "      <td>06:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 best model saved with loss of 0.008088728412985802\n"
     ]
    }
   ],
   "source": [
    "learn.fit(\n",
    "    num_epochs=10,\n",
    "    scheduler=scheduler,\n",
    "    step_scheduler_on_batch=False,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    best_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate Model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a method on the learner called `get_val_preds`, which makes predictions on the validation data. You can then use this to analyze your model's performance in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_dict = learn.get_val_preds(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': {'y_true': array([4, 2, 2, ..., 2, 1, 2]),\n",
       "  'y_pred': array([[0.00402687, 0.00640622, 0.00899468, 0.00694958, 0.9736226 ],\n",
       "         [0.00183889, 0.00166438, 0.9929559 , 0.00185515, 0.0016856 ],\n",
       "         [0.21054211, 0.038667  , 0.65535367, 0.07843401, 0.01700323],\n",
       "         ...,\n",
       "         [0.00215105, 0.00220033, 0.99170154, 0.00226145, 0.00168571],\n",
       "         [0.10741836, 0.7006959 , 0.05383366, 0.04526137, 0.09279067],\n",
       "         [0.00235924, 0.00255873, 0.99062705, 0.00258605, 0.00186897]],\n",
       "        dtype=float32)},\n",
       " 'season': {'y_true': array([0, 2, 2, ..., 1, 3, 2]),\n",
       "  'y_pred': array([[0.09411913, 0.04445902, 0.28530663, 0.57611525],\n",
       "         [0.32891008, 0.06075584, 0.36052248, 0.24981157],\n",
       "         [0.09471669, 0.04817087, 0.2728372 , 0.5842753 ],\n",
       "         ...,\n",
       "         [0.08327069, 0.04614794, 0.27187034, 0.598711  ],\n",
       "         [0.05318563, 0.06048654, 0.1256119 , 0.76071596],\n",
       "         [0.08759815, 0.04440476, 0.25398678, 0.61401033]], dtype=float32)}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Export Model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are happy with our training we can save (or export) our model, using the `save` method (or `export`).\n",
    "\n",
    "See the docs for the difference between `save` and `export`.\n",
    "\n",
    "We will need the saved model later to use in the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(folder='/home/ec2-user/fashion_dataset/models/', model_id='TEXT_MODEL1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(folder='/home/ec2-user/fashion_dataset/models/', model_id='TEXT_MODEL1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an image model and a text model, we can move to `Step4_train_ensemble_model`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
