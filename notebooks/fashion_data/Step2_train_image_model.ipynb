{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the second step of this tutorial, we will train an image model. This step can be run in parallel with Step 3 (training the text model).\n",
    "\n",
    "This notebook was run on an AWS p3.2xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Octopod Image Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for images, we use the MultiInputMultiTaskLearner since we will send in the full image and a center crop of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octopod import MultiInputMultiTaskLearner, MultiDatasetLoader\n",
    "from octopod.vision.dataset import OctopodImageDataset\n",
    "from octopod.vision.models import ResnetForMultiTaskClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in train and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the csv's we created in Step 1.\n",
    "Remember to change the path if you stored your data somewhere other than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_GENDER_DF = pd.read_csv('/home/ec2-user/fashion_dataset/gender_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_GENDER_DF = pd.read_csv('/home/ec2-user/fashion_dataset/gender_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SEASON_DF = pd.read_csv('/home/ec2-user/fashion_dataset/season_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SEASON_DF = pd.read_csv('/home/ec2-user/fashion_dataset/season_valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will most likely have to alter this to however big your batches can be on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `OctopodImageDataSet` class to create train and valid datasets for each task.\n",
    "\n",
    "Check out the documentation for infomation about the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_train_dataset = OctopodImageDataset(\n",
    "    x=TRAIN_GENDER_DF['image_urls'],\n",
    "    y=TRAIN_GENDER_DF['gender_cat'],\n",
    "    transform='train',\n",
    "    crop_transform='train'\n",
    ")\n",
    "gender_valid_dataset = OctopodImageDataset(\n",
    "    x=VALID_GENDER_DF['image_urls'],\n",
    "    y=VALID_GENDER_DF['gender_cat'],\n",
    "    transform='val',\n",
    "    crop_transform='val'\n",
    ")\n",
    "\n",
    "season_train_dataset = OctopodImageDataset(\n",
    "    x=TRAIN_SEASON_DF['image_urls'],\n",
    "    y=TRAIN_SEASON_DF['season_cat'],\n",
    "    transform='train',\n",
    "    crop_transform='train'\n",
    ")\n",
    "season_valid_dataset = OctopodImageDataset(\n",
    "    x=VALID_SEASON_DF['image_urls'],\n",
    "    y=VALID_SEASON_DF['season_cat'],\n",
    "    transform='val',\n",
    "    crop_transform='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then put the datasets into a dictionary of dataloaders.\n",
    "\n",
    "Each task is a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders_dict = {\n",
    "    'gender': DataLoader(gender_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    'season': DataLoader(season_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "}\n",
    "valid_dataloaders_dict = {\n",
    "    'gender': DataLoader(gender_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4),\n",
    "    'season': DataLoader(season_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary of dataloaders is then put into an instance of the Octopod `MultiDatasetLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLoader = MultiDatasetLoader(loader_dict=train_dataloaders_dict)\n",
    "len(TrainLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ValidLoader = MultiDatasetLoader(loader_dict=valid_dataloaders_dict, shuffle=False)\n",
    "len(ValidLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a dictionary of the tasks and the number of unique values so that we can create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_task_dict = {\n",
    "    'gender': TRAIN_GENDER_DF['gender_cat'].nunique(),\n",
    "    'season': TRAIN_SEASON_DF['season_cat'].nunique(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': 5, 'season': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_task_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model and Learner\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are completely new tasks so we use `new_task_dict`. If we had already trained a model on some tasks, we would use `pretrained_task_dict`.\n",
    "\n",
    "And since these are new tasks, we set `load_pretrained_renset=True` to use the weights from Torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResnetForMultiTaskClassification(\n",
    "    new_task_dict=new_task_dict,\n",
    "    load_pretrained_resnet=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will likely need to explore different values in this section to find some that work\n",
    "for your particular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_last = 1e-2\n",
    "lr_main = 1e-4\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.resnet.parameters(), 'lr': lr_main},\n",
    "    {'params': model.dense_layers.parameters(), 'lr': lr_last},\n",
    "    {'params': model.new_classifiers.parameters(), 'lr': lr_last},\n",
    "    \n",
    "])\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size= 4, gamma= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function_dict = {'gender': 'categorical_cross_entropy', 'season': 'categorical_cross_entropy'}\n",
    "metric_function_dict = {'gender': 'multi_class_acc', 'season': 'multi_class_acc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = MultiInputMultiTaskLearner(model, TrainLoader, ValidLoader, new_task_dict, loss_function_dict, metric_function_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your model trains, you can see some output of how the model is performing overall and how it is doing on each individual task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>gender_train_loss</th>\n",
       "      <th>gender_val_loss</th>\n",
       "      <th>gender_multi_class_accuracy</th>\n",
       "      <th>season_train_loss</th>\n",
       "      <th>season_val_loss</th>\n",
       "      <th>season_multi_class_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.703006</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.555483</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>0.870539</td>\n",
       "      <td>0.899794</td>\n",
       "      <td>0.012841</td>\n",
       "      <td>0.674125</td>\n",
       "      <td>05:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.591430</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.429940</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.865248</td>\n",
       "      <td>0.806850</td>\n",
       "      <td>0.013021</td>\n",
       "      <td>0.656405</td>\n",
       "      <td>05:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.560801</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.400612</td>\n",
       "      <td>0.005272</td>\n",
       "      <td>0.886863</td>\n",
       "      <td>0.774485</td>\n",
       "      <td>0.012343</td>\n",
       "      <td>0.678781</td>\n",
       "      <td>05:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.536615</td>\n",
       "      <td>0.007883</td>\n",
       "      <td>0.373839</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.882922</td>\n",
       "      <td>0.753750</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.707764</td>\n",
       "      <td>05:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.460809</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.295338</td>\n",
       "      <td>0.004119</td>\n",
       "      <td>0.909715</td>\n",
       "      <td>0.681539</td>\n",
       "      <td>0.010804</td>\n",
       "      <td>0.718426</td>\n",
       "      <td>05:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.430903</td>\n",
       "      <td>0.006967</td>\n",
       "      <td>0.265839</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>0.911967</td>\n",
       "      <td>0.651090</td>\n",
       "      <td>0.010886</td>\n",
       "      <td>0.717525</td>\n",
       "      <td>05:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.420281</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.254249</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>0.913430</td>\n",
       "      <td>0.641759</td>\n",
       "      <td>0.010977</td>\n",
       "      <td>0.719628</td>\n",
       "      <td>05:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.405962</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>0.242168</td>\n",
       "      <td>0.004057</td>\n",
       "      <td>0.914331</td>\n",
       "      <td>0.624455</td>\n",
       "      <td>0.011189</td>\n",
       "      <td>0.718877</td>\n",
       "      <td>05:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.390391</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.227056</td>\n",
       "      <td>0.003962</td>\n",
       "      <td>0.918496</td>\n",
       "      <td>0.608270</td>\n",
       "      <td>0.010813</td>\n",
       "      <td>0.732392</td>\n",
       "      <td>05:17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.385020</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.223193</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.917145</td>\n",
       "      <td>0.600888</td>\n",
       "      <td>0.011325</td>\n",
       "      <td>0.719177</td>\n",
       "      <td>05:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 best model saved with loss of 0.0068972790613770485\n"
     ]
    }
   ],
   "source": [
    "learn.fit(\n",
    "    num_epochs=10,\n",
    "    scheduler=exp_lr_scheduler,\n",
    "    step_scheduler_on_batch=False,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    best_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the above cell and see an error like: \n",
    "\n",
    "```python\n",
    "RuntimeError: DataLoader worker (pid X) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.\n",
    "```\n",
    "\n",
    "Try lowering the `num_workers` to `0` for each `DataLoader` in `train_dataloaders_dict` and `valid_dataloaders_dict`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a method on the learner called `get_val_preds`, which makes predictions on the validation data. You can then use this to analyze your model's performance in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_dict = learn.get_val_preds(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': {'y_true': array([4, 2, 2, ..., 2, 1, 2]),\n",
       "  'y_pred': array([[1.0643114e-05, 1.9448551e-05, 6.4263893e-03, 2.4277097e-05,\n",
       "          9.9351925e-01],\n",
       "         [3.7855210e-05, 9.4908665e-07, 9.7912544e-01, 1.1448457e-02,\n",
       "          9.3874158e-03],\n",
       "         [4.9515556e-06, 1.6856543e-08, 9.9999058e-01, 1.9361635e-06,\n",
       "          2.5575107e-06],\n",
       "         ...,\n",
       "         [4.7423075e-08, 6.8862270e-11, 9.9994063e-01, 3.9694965e-05,\n",
       "          1.9628200e-05],\n",
       "         [6.6133044e-03, 9.7994930e-01, 3.5375298e-05, 7.4712734e-05,\n",
       "          1.3327226e-02],\n",
       "         [1.0463706e-04, 1.4543773e-06, 9.8523051e-01, 9.5783398e-03,\n",
       "          5.0851600e-03]], dtype=float32)},\n",
       " 'season': {'y_true': array([0, 2, 2, ..., 1, 3, 2]),\n",
       "  'y_pred': array([[6.14131868e-01, 1.21174764e-03, 3.72315139e-01, 1.23412330e-02],\n",
       "         [2.93509573e-01, 2.31928546e-02, 6.80059910e-01, 3.23756272e-03],\n",
       "         [2.14383863e-02, 3.07431183e-04, 9.71018910e-01, 7.23517640e-03],\n",
       "         ...,\n",
       "         [4.41301949e-02, 7.47406564e-04, 9.44657326e-01, 1.04650818e-02],\n",
       "         [1.46861905e-02, 7.57379225e-03, 4.70951125e-02, 9.30644929e-01],\n",
       "         [5.96364796e-01, 1.67037919e-03, 3.76314670e-01, 2.56501492e-02]],\n",
       "        dtype=float32)}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Export Model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are happy with our training we can save (or export) our model, using the `save` method (or `export`).\n",
    "\n",
    "See the docs for the difference between `save` and `export`.\n",
    "\n",
    "We will need the saved model later to use in the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(folder='/home/ec2-user/fashion_dataset/models/', model_id='IMAGE_MODEL1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(folder='/home/ec2-user/fashion_dataset/models/', model_id='IMAGE_MODEL1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an image model, we can move to `Step3_train_text_model`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
