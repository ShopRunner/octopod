{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the second step of this tutorial, we will train an image model. This step can be run in parallel with Step 3 (training the text model).\n",
    "\n",
    "This notebook was run on an AWS p3.2xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Octopod Image Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for images, we use the MultiInputMultiTaskLearner since we will send in the full image and a center crop of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from octopod import MultiInputMultiTaskLearner, MultiDatasetLoader\n",
    "from octopod.vision.dataset import OctopodImageDataset\n",
    "from octopod.vision.models import ResnetForMultiTaskClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in train and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the csv's we created in Step 1.\n",
    "Remember to change the path if you stored your data somewhere other than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_GENDER_DF = pd.read_csv('/home/ubuntu/fashion_dataset/gender_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_GENDER_DF = pd.read_csv('/home/ubuntu/fashion_dataset/gender_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SEASON_DF = pd.read_csv('/home/ubuntu/fashion_dataset/season_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SEASON_DF = pd.read_csv('/home/ubuntu/fashion_dataset/season_valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will most likely have to alter this to however big your batches can be on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `OctopodImageDataSet` class to create train and valid datasets for each task.\n",
    "\n",
    "Check out the documentation for infomation about the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_train_dataset = OctopodImageDataset(\n",
    "    x=TRAIN_GENDER_DF['image_urls'],\n",
    "    y=TRAIN_GENDER_DF['gender_cat'],\n",
    "    transform='train',\n",
    "    crop_transform='train'\n",
    ")\n",
    "gender_valid_dataset = OctopodImageDataset(\n",
    "    x=VALID_GENDER_DF['image_urls'],\n",
    "    y=VALID_GENDER_DF['gender_cat'],\n",
    "    transform='val',\n",
    "    crop_transform='val'\n",
    ")\n",
    "\n",
    "season_train_dataset = OctopodImageDataset(\n",
    "    x=TRAIN_SEASON_DF['image_urls'],\n",
    "    y=TRAIN_SEASON_DF['season_cat'],\n",
    "    transform='train',\n",
    "    crop_transform='train'\n",
    ")\n",
    "season_valid_dataset = OctopodImageDataset(\n",
    "    x=VALID_SEASON_DF['image_urls'],\n",
    "    y=VALID_SEASON_DF['season_cat'],\n",
    "    transform='val',\n",
    "    crop_transform='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then put the datasets into a dictionary of dataloaders.\n",
    "\n",
    "Each task is a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders_dict = {\n",
    "    'gender': DataLoader(gender_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    'season': DataLoader(season_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "}\n",
    "valid_dataloaders_dict = {\n",
    "    'gender': DataLoader(gender_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4),\n",
    "    'season': DataLoader(season_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary of dataloaders is then put into an instance of the Octopod `MultiDatasetLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLoader = MultiDatasetLoader(loader_dict=train_dataloaders_dict)\n",
    "len(TrainLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ValidLoader = MultiDatasetLoader(loader_dict=valid_dataloaders_dict, shuffle=False)\n",
    "len(ValidLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a dictionary of the tasks and the number of unique values so that we can create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_task_dict = {\n",
    "    'gender': TRAIN_GENDER_DF['gender_cat'].nunique(),\n",
    "    'season': TRAIN_SEASON_DF['season_cat'].nunique(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': 5, 'season': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_task_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model and Learner\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are completely new tasks so we use `new_task_dict`. If we had already trained a model on some tasks, we would use `pretrained_task_dict`.\n",
    "\n",
    "And since these are new tasks, we set `load_pretrained_renset=True` to use the weights from Torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResnetForMultiTaskClassification(\n",
    "    new_task_dict=new_task_dict,\n",
    "    load_pretrained_resnet=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will likely need to explore different values in this section to find some that work\n",
    "for your particular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_last = 1e-2\n",
    "lr_main = 1e-4\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.resnet.parameters(), 'lr': lr_main},\n",
    "    {'params': model.dense_layers.parameters(), 'lr': lr_last},\n",
    "    {'params': model.new_classifiers.parameters(), 'lr': lr_last},\n",
    "    \n",
    "])\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size= 4, gamma= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = MultiInputMultiTaskLearner(model, TrainLoader, ValidLoader, new_task_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your model trains, you can see some output of how the model is performing overall and how it is doing on each individual task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>gender_train_loss</th>\n",
       "      <th>gender_val_loss</th>\n",
       "      <th>gender_acc</th>\n",
       "      <th>season_train_loss</th>\n",
       "      <th>season_val_loss</th>\n",
       "      <th>season_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.710134</td>\n",
       "      <td>0.587723</td>\n",
       "      <td>0.558538</td>\n",
       "      <td>0.425387</td>\n",
       "      <td>0.842621</td>\n",
       "      <td>0.912374</td>\n",
       "      <td>0.804276</td>\n",
       "      <td>0.663163</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.594605</td>\n",
       "      <td>0.565996</td>\n",
       "      <td>0.435231</td>\n",
       "      <td>0.346161</td>\n",
       "      <td>0.880446</td>\n",
       "      <td>0.807223</td>\n",
       "      <td>0.859252</td>\n",
       "      <td>0.661511</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.557713</td>\n",
       "      <td>0.510187</td>\n",
       "      <td>0.400228</td>\n",
       "      <td>0.341952</td>\n",
       "      <td>0.887651</td>\n",
       "      <td>0.767811</td>\n",
       "      <td>0.734610</td>\n",
       "      <td>0.694098</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.531409</td>\n",
       "      <td>0.496355</td>\n",
       "      <td>0.370840</td>\n",
       "      <td>0.301714</td>\n",
       "      <td>0.898795</td>\n",
       "      <td>0.745623</td>\n",
       "      <td>0.756002</td>\n",
       "      <td>0.679231</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.454953</td>\n",
       "      <td>0.442471</td>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.265934</td>\n",
       "      <td>0.910954</td>\n",
       "      <td>0.682738</td>\n",
       "      <td>0.677969</td>\n",
       "      <td>0.723232</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.427078</td>\n",
       "      <td>0.446454</td>\n",
       "      <td>0.260034</td>\n",
       "      <td>0.271331</td>\n",
       "      <td>0.909040</td>\n",
       "      <td>0.649927</td>\n",
       "      <td>0.680066</td>\n",
       "      <td>0.724283</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.414545</td>\n",
       "      <td>0.458448</td>\n",
       "      <td>0.246945</td>\n",
       "      <td>0.287083</td>\n",
       "      <td>0.907914</td>\n",
       "      <td>0.638137</td>\n",
       "      <td>0.687045</td>\n",
       "      <td>0.726536</td>\n",
       "      <td>05:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.401254</td>\n",
       "      <td>0.444956</td>\n",
       "      <td>0.234169</td>\n",
       "      <td>0.266389</td>\n",
       "      <td>0.913543</td>\n",
       "      <td>0.624160</td>\n",
       "      <td>0.683162</td>\n",
       "      <td>0.725184</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.384518</td>\n",
       "      <td>0.444607</td>\n",
       "      <td>0.219407</td>\n",
       "      <td>0.268102</td>\n",
       "      <td>0.915119</td>\n",
       "      <td>0.604791</td>\n",
       "      <td>0.680062</td>\n",
       "      <td>0.730440</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.381601</td>\n",
       "      <td>0.450801</td>\n",
       "      <td>0.216376</td>\n",
       "      <td>0.273748</td>\n",
       "      <td>0.915119</td>\n",
       "      <td>0.602026</td>\n",
       "      <td>0.686988</td>\n",
       "      <td>0.734344</td>\n",
       "      <td>05:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 best model saved with loss of 0.44247129835929516\n"
     ]
    }
   ],
   "source": [
    "learn.fit(\n",
    "    num_epochs=10,\n",
    "    loss_function=loss_function,\n",
    "    scheduler=exp_lr_scheduler,\n",
    "    step_scheduler_on_batch=False,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    best_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a method on the learner called `get_val_preds`, which makes predictions on the validation data. You can then use this to analyze your model's performance in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_dict = learn.get_val_preds(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': {'y_true': array([0., 2., 4., ..., 4., 2., 2.]),\n",
       "  'y_pred': array([[9.70583081e-01, 3.90860997e-03, 1.77693740e-02, 7.35662552e-03,\n",
       "          3.82295839e-04],\n",
       "         [4.52358545e-05, 6.18357763e-06, 9.99141693e-01, 1.09081397e-04,\n",
       "          6.97762414e-04],\n",
       "         [1.07502425e-03, 6.15622289e-02, 1.11506842e-02, 4.91130129e-02,\n",
       "          8.77098978e-01],\n",
       "         ...,\n",
       "         [1.22738214e-07, 5.45594521e-05, 4.20508741e-06, 4.68803449e-07,\n",
       "          9.99940634e-01],\n",
       "         [2.34877516e-04, 1.46071898e-06, 9.30457532e-01, 5.25180809e-03,\n",
       "          6.40542805e-02],\n",
       "         [4.02231701e-04, 8.23197115e-06, 9.98965979e-01, 1.17490272e-04,\n",
       "          5.05982141e-04]])},\n",
       " 'season': {'y_true': array([0., 0., 2., ..., 2., 3., 3.]),\n",
       "  'y_pred': array([[6.54517949e-01, 9.78528988e-04, 3.37896675e-01, 6.60690200e-03],\n",
       "         [1.42849788e-01, 1.24642171e-03, 8.44931781e-01, 1.09719848e-02],\n",
       "         [6.80897236e-02, 9.93176131e-04, 9.22244608e-01, 8.67245998e-03],\n",
       "         ...,\n",
       "         [1.76703826e-01, 6.56758547e-02, 2.45546699e-01, 5.12073696e-01],\n",
       "         [1.50335825e-03, 6.99771568e-04, 6.34792913e-03, 9.91448939e-01],\n",
       "         [7.61575578e-03, 1.52080331e-03, 5.40876240e-02, 9.36775923e-01]])}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Export Model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are happy with our training we can save (or export) our model, using the `save` method (or `export`).\n",
    "\n",
    "See the docs for the difference between `save` and `export`.\n",
    "\n",
    "We will need the saved model later to use in the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(folder='/home/ubuntu/fashion_dataset/models/', model_id='IMAGE_MODEL1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(folder='/home/ubuntu/fashion_dataset/models/', model_id='IMAGE_MODEL1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an image model, we can move to `Step3_train_text_model`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
