{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternate way to handle this problem is to train two separate resnet models for the two image tasks. We can then use Tonks to combine them into an ensemble model with the text model that was trained on both tasks.\n",
    "\n",
    "This notebook trains a gender model, Step6 trains a season model, but they could be run in parallel.\n",
    "\n",
    "This notebook was run on an AWS p3.2xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tonks Image Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for images, we use the MultiInputMultiTaskLearner since we will send in the full image and a center crop of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tonks import MultiInputMultiTaskLearner, MultiDatasetLoader\n",
    "from tonks.vision.dataset import TonksImageDataset\n",
    "from tonks.vision.models import ResnetForMultiTaskClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in train and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the csv's we created in Step 1.\n",
    "Remember to change the path if you stored your data somewhere other than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_GENDER_DF = pd.read_csv('/home/ubuntu/fashion_dataset/gender_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_GENDER_DF = pd.read_csv('/home/ubuntu/fashion_dataset/gender_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN_SEASON_DF = pd.read_csv('/home/ubuntu/fashion_dataset/season_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALID_SEASON_DF = pd.read_csv('/home/ubuntu/fashion_dataset/season_valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will most likely have to alter this to however big your batches can be on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `TonksImageDataSet` class to create train and valid datasets for each task.\n",
    "\n",
    "Check out the documentation for infomation about the transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_train_dataset = TonksImageDataset(\n",
    "    x=TRAIN_GENDER_DF['image_urls'],\n",
    "    y=TRAIN_GENDER_DF['gender_cat'],\n",
    "    transform='train',\n",
    "    crop_transform='train'\n",
    ")\n",
    "gender_valid_dataset = TonksImageDataset(\n",
    "    x=VALID_GENDER_DF['image_urls'],\n",
    "    y=VALID_GENDER_DF['gender_cat'],\n",
    "    transform='val',\n",
    "    crop_transform='val'\n",
    ")\n",
    "\n",
    "# season_train_dataset = TonksImageDataset(\n",
    "#     x=TRAIN_SEASON_DF['image_urls'],\n",
    "#     y=TRAIN_SEASON_DF['season_cat'],\n",
    "#     transform='train',\n",
    "#     crop_transform='train'\n",
    "# )\n",
    "# season_valid_dataset = TonksImageDataset(\n",
    "#     x=VALID_SEASON_DF['image_urls'],\n",
    "#     y=VALID_SEASON_DF['season_cat'],\n",
    "#     transform='val',\n",
    "#     crop_transform='val'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then put the datasets into a dictionary of dataloaders.\n",
    "\n",
    "Each task is a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders_dict = {\n",
    "    'gender': DataLoader(gender_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "    #'season': DataLoader(season_train_dataset, batch_size=batch_size, shuffle=True, num_workers=4),\n",
    "}\n",
    "valid_dataloaders_dict = {\n",
    "    'gender': DataLoader(gender_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4),\n",
    "    #'season': DataLoader(season_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary of dataloaders is then put into an instance of the Tonks `MultiDatasetLoader` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "417"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLoader = MultiDatasetLoader(loader_dict=train_dataloaders_dict)\n",
    "len(TrainLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ValidLoader = MultiDatasetLoader(loader_dict=valid_dataloaders_dict, shuffle=False)\n",
    "len(ValidLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a dictionary of the tasks and the number of unique values so that we can create our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_task_dict = {\n",
    "    'gender': TRAIN_GENDER_DF['gender_cat'].nunique(),\n",
    "    #'season': TRAIN_SEASON_DF['season_cat'].nunique(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': 5}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_task_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model and Learner\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are completely new tasks so we use `new_task_dict`. If we had already trained a model on some tasks, we would use `pretrained_task_dict`.\n",
    "\n",
    "And since these are new tasks, we set `load_pretrained_renset=True` to use the weights from Torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResnetForMultiTaskClassification(\n",
    "    new_task_dict=new_task_dict,\n",
    "    load_pretrained_resnet=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will likely need to explore different values in this section to find some that work\n",
    "for your particular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_last = 1e-2\n",
    "lr_main = 1e-4\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.resnet.parameters(), 'lr': lr_main},\n",
    "    {'params': model.dense_layers.parameters(), 'lr': lr_last},\n",
    "    {'params': model.new_classifiers.parameters(), 'lr': lr_last},\n",
    "    \n",
    "])\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size= 4, gamma= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = MultiInputMultiTaskLearner(model, TrainLoader, ValidLoader, new_task_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your model trains, you can see some output of how the model is performing overall and how it is doing on each individual task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>gender_train_loss</th>\n",
       "      <th>gender_val_loss</th>\n",
       "      <th>gender_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.545429</td>\n",
       "      <td>0.384922</td>\n",
       "      <td>0.545429</td>\n",
       "      <td>0.384922</td>\n",
       "      <td>0.856242</td>\n",
       "      <td>03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.422422</td>\n",
       "      <td>0.360401</td>\n",
       "      <td>0.422422</td>\n",
       "      <td>0.360401</td>\n",
       "      <td>0.877969</td>\n",
       "      <td>03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.381229</td>\n",
       "      <td>0.306682</td>\n",
       "      <td>0.381229</td>\n",
       "      <td>0.306682</td>\n",
       "      <td>0.891703</td>\n",
       "      <td>03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.351055</td>\n",
       "      <td>0.304995</td>\n",
       "      <td>0.351055</td>\n",
       "      <td>0.304995</td>\n",
       "      <td>0.891478</td>\n",
       "      <td>03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.274421</td>\n",
       "      <td>0.278377</td>\n",
       "      <td>0.274421</td>\n",
       "      <td>0.278377</td>\n",
       "      <td>0.904199</td>\n",
       "      <td>03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.248135</td>\n",
       "      <td>0.271745</td>\n",
       "      <td>0.248135</td>\n",
       "      <td>0.271745</td>\n",
       "      <td>0.913092</td>\n",
       "      <td>03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.234877</td>\n",
       "      <td>0.278151</td>\n",
       "      <td>0.234877</td>\n",
       "      <td>0.278151</td>\n",
       "      <td>0.910165</td>\n",
       "      <td>03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.220103</td>\n",
       "      <td>0.265007</td>\n",
       "      <td>0.220103</td>\n",
       "      <td>0.265007</td>\n",
       "      <td>0.916695</td>\n",
       "      <td>03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.210623</td>\n",
       "      <td>0.279093</td>\n",
       "      <td>0.210623</td>\n",
       "      <td>0.279093</td>\n",
       "      <td>0.913655</td>\n",
       "      <td>03:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.204511</td>\n",
       "      <td>0.283927</td>\n",
       "      <td>0.204511</td>\n",
       "      <td>0.283927</td>\n",
       "      <td>0.910841</td>\n",
       "      <td>03:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 best model saved with loss of 0.26500736598185853\n"
     ]
    }
   ],
   "source": [
    "learn.fit(\n",
    "    num_epochs=10,\n",
    "    loss_function=loss_function,\n",
    "    scheduler=exp_lr_scheduler,\n",
    "    step_scheduler_on_batch=False,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    best_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a method on the learner called `get_val_preds`, which makes predictions on the validation data. You can then use this to analyze your model's performance in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_dict = learn.get_val_preds(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': {'y_true': array([0., 2., 4., ..., 4., 2., 2.]),\n",
       "  'y_pred': array([[9.73542154e-01, 5.11498132e-04, 2.28543524e-02, 3.06490995e-03,\n",
       "          2.71824210e-05],\n",
       "         [2.53702041e-07, 1.22295772e-12, 9.99999762e-01, 1.26273869e-09,\n",
       "          3.11810799e-10],\n",
       "         [6.01408137e-05, 2.56420840e-02, 3.61046230e-04, 9.01549682e-03,\n",
       "          9.64921176e-01],\n",
       "         ...,\n",
       "         [2.13427956e-10, 3.70056364e-06, 2.20753691e-07, 2.38139705e-06,\n",
       "          9.99993682e-01],\n",
       "         [2.18814967e-05, 8.69638228e-08, 9.99140859e-01, 7.30978325e-04,\n",
       "          1.06182444e-04],\n",
       "         [8.76321610e-06, 3.01653209e-08, 9.99982238e-01, 1.39757049e-07,\n",
       "          8.83442226e-06]])}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Export Model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are happy with our training we can save (or export) our model, using the `save` method (or `export`).\n",
    "\n",
    "See the docs for the difference between `save` and `export`.\n",
    "\n",
    "We will need the saved model later to use in the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(folder='/home/ubuntu/fashion_dataset/models/', model_id='GENDER_IMAGE_MODEL1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(folder='/home/ubuntu/fashion_dataset/models/', model_id='GENDER_IMAGE_MODEL1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a gender image model, we can move to `Step6_train_season_image_model`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
