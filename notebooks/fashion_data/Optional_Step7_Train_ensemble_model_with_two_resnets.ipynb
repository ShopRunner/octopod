{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tonks Ensemble Model Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the seventh step of this tutorial, we will train an ensemble model using the two image models and one text model that we already trained.\n",
    "\n",
    "This notebook was run on an AWS p3.2xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, BertTokenizer, get_cosine_schedule_with_warmup\n",
    "\n",
    "from tonks.learner import MultiTaskLearner, MultiInputMultiTaskLearner\n",
    "from tonks.dataloader import MultiDatasetLoader\n",
    "from tonks.ensemble import TonksEnsembleDataset, BertResnetEnsembleForMultiTaskClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in train and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in the csv's we created in Step 1.\n",
    "Remember to change the path if you stored your data somewhere other than the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_GENDER_DF = pd.read_csv('/home/ubuntu/fashion_dataset/gender_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_GENDER_DF = pd.read_csv('/home/ubuntu/fashion_dataset/gender_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SEASON_DF = pd.read_csv('/home/ubuntu/fashion_dataset/season_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SEASON_DF = pd.read_csv('/home/ubuntu/fashion_dataset/season_valid.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will most likely have to alter this to however big your batches can be on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tok = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case=True\n",
    ")\n",
    "\n",
    "max_seq_length = 128 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_train_dataset = TonksEnsembleDataset(\n",
    "    text_inputs=TRAIN_GENDER_DF['productDisplayName'],\n",
    "    img_inputs=TRAIN_GENDER_DF['image_urls'],\n",
    "    y=TRAIN_GENDER_DF['gender_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length,\n",
    "    transform='train',\n",
    "    crop_transform='train'\n",
    "\n",
    ")\n",
    "gender_valid_dataset = TonksEnsembleDataset(\n",
    "    text_inputs=VALID_GENDER_DF['productDisplayName'],\n",
    "    img_inputs=VALID_GENDER_DF['image_urls'],\n",
    "    y=VALID_GENDER_DF['gender_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length,\n",
    "    transform='val',\n",
    "    crop_transform='val'\n",
    "\n",
    ")\n",
    "\n",
    "season_train_dataset = TonksEnsembleDataset(\n",
    "    text_inputs=TRAIN_SEASON_DF['productDisplayName'],\n",
    "    img_inputs=TRAIN_SEASON_DF['image_urls'],\n",
    "    y=TRAIN_SEASON_DF['season_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length,\n",
    "    transform='train',\n",
    "    crop_transform='train'\n",
    "\n",
    ")\n",
    "season_valid_dataset = TonksEnsembleDataset(\n",
    "    text_inputs=VALID_SEASON_DF['productDisplayName'],\n",
    "    img_inputs=VALID_SEASON_DF['image_urls'],\n",
    "    y=VALID_SEASON_DF['season_cat'],\n",
    "    tokenizer=bert_tok,\n",
    "    max_seq_length=max_seq_length,\n",
    "    transform='val',\n",
    "    crop_transform='val'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then put the datasets into a dictionary of dataloaders.\n",
    "\n",
    "Each task is a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders_dict = {\n",
    "    'gender': DataLoader(gender_train_dataset, batch_size=batch_size, shuffle=True, num_workers=6),\n",
    "    'season': DataLoader(season_train_dataset, batch_size=batch_size, shuffle=True, num_workers=6),\n",
    "}\n",
    "valid_dataloaders_dict = {\n",
    "    'gender': DataLoader(gender_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=6),\n",
    "    'season': DataLoader(season_valid_dataset, batch_size=batch_size, shuffle=False, num_workers=6),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainLoader = MultiDatasetLoader(loader_dict=train_dataloaders_dict)\n",
    "len(TrainLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ValidLoader = MultiDatasetLoader(\n",
    "    loader_dict=valid_dataloaders_dict,\n",
    "    shuffle=False\n",
    ")\n",
    "len(ValidLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Model and Learner\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the image model could potentially have multiple Resnets for different subsets of tasks, we need to create an `image_task_dict` that splits up the tasks grouped by the Resnet they use.\n",
    "\n",
    "This version uses different resnets for gender and season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_task_dict = {\n",
    "    'gender': {\n",
    "        'gender': TRAIN_GENDER_DF['gender_cat'].nunique()    \n",
    "    },\n",
    "    'season': {\n",
    "        'season': TRAIN_SEASON_DF['season_cat'].nunique()\n",
    "    }  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to create the `new_task_dict` for the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_task_dict = {\n",
    "    'gender': TRAIN_GENDER_DF['gender_cat'].nunique(),\n",
    "    'season': TRAIN_SEASON_DF['season_cat'].nunique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize the model by setting up the right shape with the image_task_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertResnetEnsembleForMultiTaskClassification(\n",
    "    image_task_dict=image_task_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load in the existing models by specifying the folder where the models live and their id's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model_id_dict = {\n",
    "    'gender': 'GENDER_IMAGE_MODEL1',\n",
    "    'season': 'SEASON_IMAGE_MODEL1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_core_models(\n",
    "    folder='/home/ubuntu/fashion_dataset/models/',\n",
    "    bert_model_id='TEXT_MODEL1',\n",
    "    resnet_model_id_dict=resnet_model_id_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've set some helper methods that will freeze the core bert and resnets for you if you only want to train the new layers. As with all other aspects of training, this is likely to require some experimentation to determine what works for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will likely need to explore different values in this section to find some that work\n",
    "for your particular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_bert()\n",
    "model.freeze_resnets()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_last = 1e-3\n",
    "lr_main = 1e-5\n",
    "\n",
    "lr_list = [\n",
    "    {'params': model.bert.parameters(), 'lr': lr_main},\n",
    "    {'params': model.dropout.parameters(), 'lr': lr_main},   \n",
    "    {'params': model.image_resnets.parameters(), 'lr': lr_main},\n",
    "    {'params': model.image_dense_layers.parameters(), 'lr': lr_main},\n",
    "    {'params': model.ensemble_layers.parameters(), 'lr': lr_last},\n",
    "    {'params': model.classifiers.parameters(), 'lr': lr_last},\n",
    "]\n",
    "\n",
    "optimizer = optim.Adam(lr_list)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size= 4, gamma= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = MultiInputMultiTaskLearner(model, TrainLoader, ValidLoader, new_task_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your model trains, you can see some output of how the model is performing overall and how it is doing on each individual task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>gender_train_loss</th>\n",
       "      <th>gender_val_loss</th>\n",
       "      <th>gender_acc</th>\n",
       "      <th>season_train_loss</th>\n",
       "      <th>season_val_loss</th>\n",
       "      <th>season_acc</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0.293111</td>\n",
       "      <td>0.322882</td>\n",
       "      <td>0.051661</td>\n",
       "      <td>0.032669</td>\n",
       "      <td>0.989643</td>\n",
       "      <td>0.615227</td>\n",
       "      <td>0.710022</td>\n",
       "      <td>0.715423</td>\n",
       "      <td>06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.266368</td>\n",
       "      <td>0.294048</td>\n",
       "      <td>0.032354</td>\n",
       "      <td>0.032366</td>\n",
       "      <td>0.990431</td>\n",
       "      <td>0.578563</td>\n",
       "      <td>0.643127</td>\n",
       "      <td>0.747259</td>\n",
       "      <td>06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.258399</td>\n",
       "      <td>0.317407</td>\n",
       "      <td>0.028151</td>\n",
       "      <td>0.043071</td>\n",
       "      <td>0.988292</td>\n",
       "      <td>0.565570</td>\n",
       "      <td>0.683366</td>\n",
       "      <td>0.732693</td>\n",
       "      <td>06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.258816</td>\n",
       "      <td>0.305413</td>\n",
       "      <td>0.026256</td>\n",
       "      <td>0.031470</td>\n",
       "      <td>0.990093</td>\n",
       "      <td>0.569071</td>\n",
       "      <td>0.670849</td>\n",
       "      <td>0.736147</td>\n",
       "      <td>06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.249474</td>\n",
       "      <td>0.296359</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.030410</td>\n",
       "      <td>0.990544</td>\n",
       "      <td>0.549775</td>\n",
       "      <td>0.651130</td>\n",
       "      <td>0.746508</td>\n",
       "      <td>06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.245043</td>\n",
       "      <td>0.289674</td>\n",
       "      <td>0.020839</td>\n",
       "      <td>0.029983</td>\n",
       "      <td>0.991332</td>\n",
       "      <td>0.544149</td>\n",
       "      <td>0.636099</td>\n",
       "      <td>0.752365</td>\n",
       "      <td>06:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.246843</td>\n",
       "      <td>0.289804</td>\n",
       "      <td>0.020975</td>\n",
       "      <td>0.031275</td>\n",
       "      <td>0.990093</td>\n",
       "      <td>0.548169</td>\n",
       "      <td>0.634677</td>\n",
       "      <td>0.749812</td>\n",
       "      <td>06:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.286017</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.029878</td>\n",
       "      <td>0.991107</td>\n",
       "      <td>0.539635</td>\n",
       "      <td>0.627703</td>\n",
       "      <td>0.752816</td>\n",
       "      <td>06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.241970</td>\n",
       "      <td>0.288154</td>\n",
       "      <td>0.020137</td>\n",
       "      <td>0.029609</td>\n",
       "      <td>0.991107</td>\n",
       "      <td>0.537913</td>\n",
       "      <td>0.633048</td>\n",
       "      <td>0.750113</td>\n",
       "      <td>06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0.240077</td>\n",
       "      <td>0.287712</td>\n",
       "      <td>0.019503</td>\n",
       "      <td>0.029321</td>\n",
       "      <td>0.991219</td>\n",
       "      <td>0.534341</td>\n",
       "      <td>0.632402</td>\n",
       "      <td>0.752816</td>\n",
       "      <td>06:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 best model saved with loss of 0.2860171677235031\n"
     ]
    }
   ],
   "source": [
    "learn.fit(\n",
    "    num_epochs=10,\n",
    "    loss_function=loss_function,\n",
    "    scheduler=exp_lr_scheduler,\n",
    "    step_scheduler_on_batch=False,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    best_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to check your specific use case to determine whether it is better to train all of your image tasks in one resnet or in multiple resnets. You won't necessarily need one resnet per task, particularly if some of your tasks are related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking validation data\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a method on the learner called `get_val_preds`, which makes predictions on the validation data. You can then use this to analyze your model's performance in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = learn.get_val_preds(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gender': {'y_true': array([0., 2., 4., ..., 4., 2., 2.]),\n",
       "  'y_pred': array([[9.92650092e-01, 3.23335262e-04, 6.87686214e-03, 1.36870003e-04,\n",
       "          1.29461450e-05],\n",
       "         [1.35707080e-06, 1.94859410e-07, 9.99997497e-01, 7.13497911e-07,\n",
       "          2.35319845e-07],\n",
       "         [7.03864571e-05, 2.32889410e-03, 4.51153865e-05, 6.30916620e-05,\n",
       "          9.97492552e-01],\n",
       "         ...,\n",
       "         [9.33719548e-07, 5.31011210e-05, 5.51037431e-07, 9.38349842e-07,\n",
       "          9.99944448e-01],\n",
       "         [1.97838494e-06, 6.37559253e-07, 9.99994874e-01, 1.81650728e-06,\n",
       "          6.87618694e-07],\n",
       "         [2.48589913e-06, 6.35381923e-07, 9.99993920e-01, 2.18791865e-06,\n",
       "          7.60277146e-07]])},\n",
       " 'season': {'y_true': array([0., 0., 2., ..., 2., 3., 3.]),\n",
       "  'y_pred': array([[8.31580341e-01, 1.52546994e-03, 1.59035668e-01, 7.85854086e-03],\n",
       "         [8.96789908e-01, 4.13205649e-04, 1.00355275e-01, 2.44156295e-03],\n",
       "         [7.72640035e-02, 3.73037672e-03, 8.75569582e-01, 4.34360504e-02],\n",
       "         ...,\n",
       "         [1.32084116e-01, 4.27775532e-02, 7.20311642e-01, 1.04826726e-01],\n",
       "         [3.08280811e-04, 1.46326784e-04, 1.01489592e-02, 9.89396393e-01],\n",
       "         [1.14652375e-02, 1.08387964e-02, 4.55902964e-02, 9.32105660e-01]])}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Export Model\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ensemble model can also be saved or exported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(folder='/home/ubuntu/fashion_dataset/models/', model_id='ENSEMBLE_MODEL2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(folder='/home/ubuntu/fashion_dataset/models/', model_id='ENSEMBLE_MODEL2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
